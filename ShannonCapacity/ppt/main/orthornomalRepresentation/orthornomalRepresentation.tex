\subsection{Orthonormal Representation}

      \begin{frame}
            \frametitle{Orthonormal Representation}

            Here we have the third way to defined a set of characters.

            \begin{definition}[Orthonormal Representation]\label{def:orthonormalRepresentation}
                  Given a graph $ G $ with vertices $ 1,2,\dots,n $, the orthonormal representation of $ G $ is a set of \textbf{unit vectors} $ \{v_1, v_2, \dots, v_n\} $ such that if $ i $ and $ j $ are \textbf{not adjacent} in $ G $ then $ v_i $ and $ v_j $ are \textbf{orthogonal}.
            \end{definition}

            \pause

            This existence of the orthonormal representation can be proved by induction.

      \end{frame}

\subsection{Tensor Product}

      \begin{frame}
            \frametitle{Tensor Product}
            \begin{definition}[tensor product]\label{def:tensorProduct}
                  Given two vectors $ v = \left(v_{1},\dots,v_{n}\right)^{T} $ and $ w = \left(w_{1},\dots,w_{n}\right)^{T} $, the tensor product $ v \circ w $ is defined by
                  \begin{equation}
                        v \circ w = \left(
                              v_{1}w_{1},\dots,v_{1}w_{n},
                              v_{2}w_{1},\dots,v_{2}w_{n},
                              \dots,
                              v_{n}w_{1},\dots,v_{n}w_{n}
                              \right)^{T}
                  \end{equation}

                  Somehow equal to stack up the columns of $ v w^{T} $.
            \end{definition}

            \pause

            \begin{lemma}
                  The inner product of tensor products can be computed by,
                  \begin{equation}
                        \langle v \circ w, v' \circ w' \rangle = \langle v, v' \rangle \langle w, w' \rangle
                  \end{equation}
            \end{lemma}

      \end{frame}

      \begin{frame}
            \frametitle{Product of Orthonormal Representation}

            \begin{lemma}
                  Given a graph $ G $ with vertices $ 1,2,\dots,n $, and a graph $ H $ with vertices $ 1,2,\dots,m $.
                  Let $ \{v_1, v_2, \dots, v_n\} $ and $ \{w_1, w_2, \dots, w_m\} $ be orthonormal representations of $ G $ and $ H $ respectively.
                  Then vectors $\{ v_{i}\circ w_{j} \}$ is an orthonormal representation of $ G \times H $.
            \end{lemma}

            \pause

            \begin{proof}
                  Let $(i,j)$ and $(i',j')$ be two vertices in $ G \times H $.

                  If $ (i,j) $ and $ (i',j') $ not adjacent, then at least one of $ i $ and $ i' $ is not adjacent to $ j $ and $ j' $ respectively, which means at least one of $ v_i $ and $ v_{i'} $ is orthogonal to $ w_j $ and $ w_{j'} $ respectively.
                  \begin{align}
                        \langle v_i \circ w_j, v_{i'} \circ w_{j'} \rangle &= \langle v_i, v_{i'} \rangle \langle w_j, w_{j'} \rangle \\
                        &= 0
                  \end{align}
            \end{proof}
      \end{frame}